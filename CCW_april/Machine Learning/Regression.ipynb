{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "14123567",
   "metadata": {},
   "source": [
    "# Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d25ed913",
   "metadata": {},
   "source": [
    "https://cloud.tencent.com/developer/article/2348505  \n",
    "https://blog.csdn.net/qq_39232265/article/details/78868487"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e29b670",
   "metadata": {},
   "source": [
    "回归模型旨在揭示自变量和因变量之间的关系。回归模型常使用均方误差$MSE$、$R^2$作为评价指标  \n",
    "__均方误差__  \n",
    "$$MSE=\\frac{1}{n}\\sum_{i=1}^{n}(y_i-\\hat{y}_i)^2$$\n",
    "__平均绝对误差__  \n",
    "$$MAE=\\frac{1}{n}\\sum_{i=1}^{n}|y_i-\\hat{y}_i|$$\n",
    "__R^2 Coefficient of Determination__  \n",
    "$$R^2=1-\\frac{\\sum_{i=1}^{n}(y_i-\\hat{y}_i)^2}{\\sum_{i=1}^{n}(y_i-\\bar{y})^2}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b553eab",
   "metadata": {},
   "source": [
    "## 线性回归模型\n",
    "适用于大规模数据集，鲁棒性低  \n",
    "模型表示为: $$y=\\beta_0+\\beta_1x_1+...+\\beta_nx_n+\\epsilon$$\n",
    "结果为：$$\\hat\\beta=(X^\\top X)^{-1}X^\\top y$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "05d4893c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fba6ced0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 假设数据\n",
    "X = torch.tensor([[1.0], [2.0], [3.0]])\n",
    "y = torch.tensor([[2.0], [4.0], [6.0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e9a09902",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义模型\n",
    "class LinearRegressionModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LinearRegressionModel, self).__init__()\n",
    "        self.linear = nn.Linear(1, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c7b136a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 初始化模型\n",
    "model = LinearRegressionModel()\n",
    "\n",
    "# 定义损失函数和优化器\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "16f2c8d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "模型参数： 1.9577192068099976 0.09611420333385468\n"
     ]
    }
   ],
   "source": [
    "# 训练模型\n",
    "for epoch in range(1000):\n",
    "    outputs = model(X)\n",
    "    loss = criterion(outputs, y)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "# 输出结果\n",
    "print(\"模型参数：\", model.linear.weight.item(), model.linear.bias.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b8ee72f",
   "metadata": {},
   "source": [
    "# 岭回归\n",
    "适用于具有多重共线性或特征筛选  \n",
    "目标函数为：$$L(\\boldsymbol{\\beta})=\\sum_{i=1}^{n}(y_i-\\boldsymbol{\\beta}x_i)^2+\\lambda\\lVert \\boldsymbol{\\beta}^\\top \\rVert_2^2$$\n",
    "结果为：$$\\hat\\beta=(X^\\top X+\\lambda I)^{-1}X^\\top y$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3ac000f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def ridge(X, y, lambdas=0.1):\n",
    "    \"\"\"\n",
    "    岭回归\n",
    "    args:\n",
    "        X - 训练数据集\n",
    "        y - 目标标签值\n",
    "        lambdas - 惩罚项系数\n",
    "   return:\n",
    "       w - 权重系数\n",
    "   \"\"\"\n",
    "    return np.linalg.inv(X.T.dot(X) + lambdas * np.eye(X.shape[1])).dot(X.T).dot(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5a1d8e00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.2192691]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "# 初始化岭回归器\n",
    "reg = Ridge(alpha=0.1, fit_intercept=False)\n",
    "# 拟合线性模型\n",
    "reg.fit(X, y)\n",
    "# 权重系数\n",
    "w = reg.coef_\n",
    "print(w)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3739c44e",
   "metadata": {},
   "source": [
    "## LASSO回归\n",
    "适用于具有多重共线性或特征筛选  \n",
    "目标函数为：$$L(\\boldsymbol{\\beta})=\\sum_{i=1}^{n}(y_i-\\boldsymbol{\\beta}x_i)^2+\\lambda\\lVert \\boldsymbol{\\beta}^\\top \\rVert_1$$\n",
    "使用坐标下降法、ADMM法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d34ed46d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lassoUseCd(X, y, lambdas=0.1, max_iter=1000, tol=1e-4):\n",
    "    \"\"\"\n",
    "    Lasso回归，使用坐标下降法（coordinate descent）\n",
    "    args:\n",
    "        X - 训练数据集\n",
    "        y - 目标标签值\n",
    "        lambdas - 惩罚项系数\n",
    "        max_iter - 最大迭代次数\n",
    "        tol - 变化量容忍值\n",
    "    return:\n",
    "        w - 权重系数\n",
    "    \"\"\"\n",
    "    # 初始化 w 为零向量\n",
    "    w = np.zeros(X.shape[1])\n",
    "    for it in range(max_iter):\n",
    "        done = True\n",
    "        # 遍历所有自变量\n",
    "        for i in range(0, len(w)):\n",
    "            # 记录上一轮系数\n",
    "            weight = w[i]\n",
    "            # 求出当前条件下的最佳系数\n",
    "            w[i] = down(X, y, w, i, lambdas)\n",
    "            # 当其中一个系数变化量未到达其容忍值，继续循环\n",
    "            if (np.abs(weight - w[i]) > tol):\n",
    "                done = False\n",
    "        # 所有系数都变化不大时，结束循环\n",
    "        if (done):\n",
    "            break\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e3623fb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def down(X, y, w, index, lambdas=0.1):\n",
    "    \"\"\"\n",
    "    cost(w) = (x1 * w1 + x2 * w2 + ... - y)^2 + ... + λ(|w1| + |w2| + ...)\n",
    "    假设 w1 是变量，这时其他的值均为常数，带入上式后，其代价函数是关于 w1 的一元二次函数，可以写成下式：\n",
    "    cost(w1) = (a * w1 + b)^2 + ... + λ|w1| + c (a,b,c,λ 均为常数)\n",
    "    => 展开后\n",
    "    cost(w1) = aa * w1^2 + 2ab * w1 + λ|w1| + c (aa,ab,c,λ 均为常数)\n",
    "    \"\"\"\n",
    "    # 展开后的二次项的系数之和\n",
    "    aa = 0\n",
    "    # 展开后的一次项的系数之和\n",
    "    ab = 0\n",
    "    for i in range(X.shape[0]):\n",
    "        # 括号内一次项的系数\n",
    "        a = X[i][index]\n",
    "        # 括号内常数项的系数\n",
    "        b = X[i][:].dot(w) - a * w[index] - y[i]\n",
    "        # 可以很容易的得到展开后的二次项的系数为括号内一次项的系数平方的和\n",
    "        aa = aa + a * a\n",
    "        # 可以很容易的得到展开后的一次项的系数为括号内一次项的系数乘以括号内常数项的和\n",
    "        ab = ab + a * b\n",
    "    # 由于是一元二次函数，当导数为零时，函数值最小值，只需要关注二次项系数、一次项系数和 λ\n",
    "    return det(aa, ab, lambdas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "927c99ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def det(aa, ab, lambdas=0.1):\n",
    "    \"\"\"\n",
    "    通过代价函数的导数求 w，当 w = 0 时，不可导\n",
    "    det(w) = 2aa * w + 2ab + λ = 0 (w > 0)\n",
    "    => w = - (2 * ab + λ) / (2 * aa)\n",
    "\n",
    "    det(w) = 2aa * w + 2ab - λ = 0 (w < 0)\n",
    "    => w = - (2 * ab - λ) / (2 * aa)\n",
    "\n",
    "    det(w) = NaN (w = 0)\n",
    "    => w = 0\n",
    "    \"\"\"\n",
    "    w = - (2 * ab + lambdas) / (2 * aa)\n",
    "    if w < 0:\n",
    "        w = - (2 * ab - lambdas) / (2 * aa)\n",
    "        if w > 0:\n",
    "            w = 0\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b866d041",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.22166667])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lassoUseCd(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "453cf5b1",
   "metadata": {},
   "source": [
    "第三方库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3fe266c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.21]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "# 初始化Lasso回归器，默认使用坐标下降法\n",
    "reg = Lasso(alpha=0.1, fit_intercept=False)\n",
    "# 拟合线性模型\n",
    "reg.fit(X, y)\n",
    "# 权重系数\n",
    "w = reg.coef_\n",
    "print(w)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a930923",
   "metadata": {},
   "source": [
    "## 弹性网络回归 Elastic Net Regression\n",
    "结合岭回归和Lasso回归，损失函数为\n",
    "$$L(\\beta)=\\sum_{i=1}^{n }(y_i-\\beta^\\top x_i)^2+\\lambda \\rho \\lVert \\beta \\rVert_1+\\frac{\\lambda(1-\\rho)}{2}\\lVert \\beta \\rVert_2^2$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "52daf8ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.1589404] [-39.61194806   5.82956778  33.78238028]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import ElasticNet\n",
    "\n",
    "# 初始化弹性网络回归器\n",
    "reg = ElasticNet(alpha=0.1, l1_ratio=0.5, fit_intercept=False)\n",
    "# 拟合线性模型\n",
    "reg.fit(X, y)\n",
    "# 权重系数\n",
    "w = reg.coef_\n",
    "print(w,b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a16ebea",
   "metadata": {},
   "source": [
    "## 多项式回归\n",
    "适用于小规模数据集，鲁棒性低  \n",
    "模型表示为: $$y=\\beta_0+\\beta_1x_1+...+\\beta_nx_n+\\epsilon$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d429314",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ff05e521",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 假设数据\n",
    "X = torch.tensor([[1.0], [2.0], [3.0], [4.0]])\n",
    "y = torch.tensor([[2.0], [3.9], [9.1], [16.2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "279ad188",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义模型\n",
    "class PolynomialRegressionModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(PolynomialRegressionModel, self).__init__()\n",
    "        self.poly = nn.Linear(1, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.poly(x ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8a27f274",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 初始化模型\n",
    "model = PolynomialRegressionModel()\n",
    "\n",
    "# 定义损失函数和优化器\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5092db67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "模型参数： 0.9667100310325623 0.5494906902313232\n"
     ]
    }
   ],
   "source": [
    "# 训练模型\n",
    "for epoch in range(1000):\n",
    "    outputs = model(X)\n",
    "    loss = criterion(outputs, y)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "# 输出结果\n",
    "print(\"模型参数：\", model.poly.weight.item(), model.poly.bias.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5a9871f",
   "metadata": {},
   "source": [
    "## 支持向量回归SVR\n",
    "适用于小规模数据集，鲁棒性高  \n",
    "构建最大化间隔的超平面：\n",
    "$$y=\\sum_{i=1}^{n}(\\alpha_i-\\alpha_i^*)x_i+b$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b57d6b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVR\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "468321e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 假设数据\n",
    "X = np.array([[1], [2], [3], [4]])\n",
    "y = np.array([2, 4, 3, 4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "02c43449",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.025658191398072287\n"
     ]
    }
   ],
   "source": [
    "model = SVR()\n",
    "model.fit(X,y)\n",
    "y_pred = model.predict(X)\n",
    "mse = mean_squared_error(y,y_pred)\n",
    "print(mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf642cf1",
   "metadata": {},
   "source": [
    "## 逻辑回归"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a68a67ff",
   "metadata": {},
   "source": [
    "对于二分类问题，y取0或1，函数表达式为\n",
    "$$\n",
    "y=\n",
    "\\begin{cases}\n",
    "0,&f(\\beta^\\top X)\\leq C \\\\\n",
    "1,&f(\\beta^\\top X)>C\n",
    "\\end{cases}\n",
    "$$\n",
    "其中f(x)为对数几率函数，表达式为\n",
    "$$\n",
    "f(x)=\\frac{1}{1+e^{-\\beta^\\top X}}\n",
    "$$\n",
    "将对数几率回归函数看作概率\n",
    "$$\n",
    "\\begin{cases}\n",
    "P(y=1|X,\\beta)=f(X)\\\\\n",
    "P(y=0|X,\\beta)=1-f(X)\n",
    "\\end{cases}\n",
    "$$\n",
    "转化为$$P(y|X,\\beta)=f(X)^y[1-f(X)]^{1-y}$$\n",
    "似然函数为$$L(\\beta)=\\prod_{i=1}^nf(x_i)^{y_i}[1-f(x_i)]^{1-y_i}$$\n",
    "损失（代价）函数$$Cost(\\beta)=-\\sum_{i=1}^n[y_i ln(f(x_i))+(1-y_i)ln(1-f(x_i))]$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9c95c2af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "c_1 = 1e-4\n",
    "c_2 = 0.9\n",
    "\n",
    "def cost(X, y, w):\n",
    "    \"\"\"\n",
    "    对数几率回归的代价函数\n",
    "    args:\n",
    "        X - 训练数据集\n",
    "        y - 目标标签值\n",
    "        w - 权重系数\n",
    "    return:\n",
    "        代价函数值\n",
    "    \"\"\"\n",
    "    power = -np.multiply(y, X.dot(w))\n",
    "    p1 = power[power <= 0]\n",
    "    p2 = -power[-power < 0]\n",
    "    # 解决 python 计算 e 的指数幂溢出的问题\n",
    "    return np.sum(np.log(1 + np.exp(p1))) + np.sum(np.log(1 + np.exp(p2)) - p2)\n",
    "\n",
    "def dcost(X, y, W):\n",
    "    \"\"\"\n",
    "    对数几率回归的代价函数的梯度\n",
    "    args:\n",
    "        X - 训练数据集\n",
    "        y - 目标标签值\n",
    "        w - 权重系数\n",
    "    return:\n",
    "        代价函数的梯度\n",
    "    \"\"\"\n",
    "    return X.T.dot(np.multiply(-y, 1 / (1 + np.exp(np.multiply(y, X.dot(w))))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "40e5c146",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select(step_low, step_high):\n",
    "    \"\"\"\n",
    "    在范围内选择一个步长，直接取中值\n",
    "    args:\n",
    "        step_low - 步长范围开始值\n",
    "        step_high - 步长范围结束值\n",
    "    return:\n",
    "        步长\n",
    "    \"\"\"\n",
    "    return (step_low + step_high) / 2\n",
    "\n",
    "def lineSearch(X, y, w, step_init, step_max):\n",
    "    \"\"\"\n",
    "    线搜索步长，使其满足 Wolfe 条件\n",
    "    args:\n",
    "        X - 训练数据集\n",
    "        y - 目标标签值\n",
    "        w - 权重系数\n",
    "        step_init - 步长初始值\n",
    "        step_max - 步长最大值\n",
    "    return:\n",
    "        步长\n",
    "    \"\"\"\n",
    "    step_i = step_init\n",
    "    step_low = step_init\n",
    "    step_high = step_max\n",
    "    i = 1\n",
    "    d = dcost(X, y, w)\n",
    "    p = direction(d)\n",
    "    while (True):\n",
    "        # 不满足充分下降条件或者后面的代价函数值大于前一个代价函数值\n",
    "        if (not sufficientDecrease(X, y, w, step_i) or (cost(X, y, w + step_i * p) >= cost(X, y, w + step_low * p) and i > 1)):\n",
    "            # 将当前步长作为搜索的右边界\n",
    "            # return search(X, y, w, step_prev, step_i)\n",
    "            step_high = step_i\n",
    "        else:\n",
    "            # 满足充分下降条件并且满足曲率条件，即已经满足了 Wolfe 条件\n",
    "            if (curvature(X, y, w, step_i)):\n",
    "                # 直接返回当前步长\n",
    "                return step_i\n",
    "            step_low = step_i\n",
    "        # 选择下一个步长\n",
    "        step_i = select(step_low, step_high)\n",
    "        i = i + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cd48e07",
   "metadata": {},
   "source": [
    "__梯度下降法__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "671064f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def direction(d):\n",
    "    \"\"\"\n",
    "    更新的方向\n",
    "    args:\n",
    "        d - 梯度\n",
    "    return:\n",
    "        更新的方向\n",
    "    \"\"\"\n",
    "    return -d\n",
    "\n",
    "def sufficientDecrease(X, y, w, step):\n",
    "    \"\"\"\n",
    "    判断是否满足充分下降条件（sufficient decrease condition）\n",
    "    args:\n",
    "        X - 训练数据集\n",
    "        y - 目标标签值\n",
    "        w - 权重系数\n",
    "        step - 步长\n",
    "    return:\n",
    "        是否满足充分下降条件\n",
    "    \"\"\"\n",
    "    d = dcost(X, y, w)\n",
    "    p = direction(d)\n",
    "    return cost(X, y, w + step * p) <= cost(X, y, w) + c_1 * step * p.T.dot(d)\n",
    "\n",
    "def curvature(X, y, w, step):\n",
    "    \"\"\"\n",
    "    判断是否满足曲率条件（curvature condition）\n",
    "    args:\n",
    "        X - 训练数据集\n",
    "        y - 目标标签值\n",
    "        w - 权重系数\n",
    "        step - 步长\n",
    "    return:\n",
    "        是否满足曲率条件\n",
    "    \"\"\"\n",
    "    d = dcost(X, y, w)\n",
    "    p = direction(d)\n",
    "    return -p.T.dot(dcost(X, y, w + step * p)) <= -c_2 * p.T.dot(d)\n",
    "\n",
    "def logisticRegressionGd(X, y, max_iter=100, tol=1e-4, step_init=0, step_max=10):\n",
    "    \"\"\"\n",
    "    对数几率回归，使用梯度下降法（gradient descent）\n",
    "    args:\n",
    "        X - 训练数据集\n",
    "        y - 目标标签值\n",
    "        max_iter - 最大迭代次数\n",
    "        tol - 变化量容忍值\n",
    "        step_init - 步长初始值\n",
    "        step_max - 步长最大值\n",
    "    return:\n",
    "        w - 权重系数\n",
    "    \"\"\"\n",
    "    # 初始化 w 为零向量\n",
    "    w = np.zeros(X.shape[1])\n",
    "    # 开始迭代\n",
    "    for it in range(max_iter):\n",
    "        # 计算梯度\n",
    "        d = dcost(X, y, w)\n",
    "        # 当梯度足够小时，结束迭代\n",
    "        if np.linalg.norm(x=d, ord=1) <= tol:\n",
    "            break\n",
    "        # 使用线搜索计算步长 \n",
    "        step = lineSearch(X, y, w, step_init, step_max)\n",
    "        # 更新权重系数 w\n",
    "        w = w + step * direction(d)\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c3ba5a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 假设数据\n",
    "X = np.array([[1], [2], [3], [4]])\n",
    "y = np.array([1, 1, 0, 0])\n",
    "logisticRegressionGd(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02d9fe67",
   "metadata": {},
   "source": [
    "__牛顿法__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0141508f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ddcost(X, y, w):\n",
    "   \"\"\"\n",
    "   对数几率回归的代价函数的黑塞矩阵\n",
    "   args:\n",
    "       X - 训练数据集\n",
    "       y - 目标标签值\n",
    "       w - 权重系数\n",
    "   return:\n",
    "       代价函数的黑塞矩阵\n",
    "   \"\"\"\n",
    "    exp = np.exp(np.multiply(y, X.dot(w)))\n",
    "    result = np.multiply(exp, 1 / np.square(1 + exp))\n",
    "    X_r = np.zeros(X.shape)\n",
    "    for i in range(X.shape[1]):\n",
    "        X_r[:, i] = np.multiply(result, X[:, i])\n",
    "    return X_r.T.dot(X)\n",
    "\n",
    "def direction(d, H):\n",
    "   \"\"\"\n",
    "   更新的方向\n",
    "   args:\n",
    "       d - 梯度\n",
    "       H - 黑塞矩阵\n",
    "   return:\n",
    "       更新的方向\n",
    "   \"\"\"\n",
    "   return - np.linalg.inv(H).dot(d)\n",
    "\n",
    "def sufficientDecrease(X, y, w, step):\n",
    "   \"\"\"\n",
    "   判断是否满足充分下降条件（sufficient decrease condition）\n",
    "   args:\n",
    "       X - 训练数据集\n",
    "       y - 目标标签值\n",
    "       w - 权重系数\n",
    "       step - 步长\n",
    "   return:\n",
    "       是否满足充分下降条件\n",
    "   \"\"\"\n",
    "    d = dcost(X, y, w)\n",
    "    H = ddcost(X, y, w)\n",
    "    p = direction(d, H)\n",
    "    return cost(X, y, w + step * p) <= cost(X, y, w) + c_1 * step * p.T.dot(d)\n",
    "\n",
    "def curvature(X, y, w, step):\n",
    "   \"\"\"\n",
    "   判断是否满足曲率条件（curvature condition）\n",
    "   args:\n",
    "       X - 训练数据集\n",
    "       y - 目标标签值\n",
    "       w - 权重系数\n",
    "       step - 步长\n",
    "   return:\n",
    "       是否满足曲率条件\n",
    "   \"\"\"\n",
    "    d = dcost(X, y, w)\n",
    "    H = ddcost(X, y, w)\n",
    "    p = direction(d, H)\n",
    "    return -p.T.dot(dcost(X, y, w + step * p)) <= -c_2 * p.T.dot(d)\n",
    "\n",
    "def lineSearch(X, y, w, step_init, step_max):\n",
    "   \"\"\"\n",
    "   线搜索步长，使其满足 Wolfe 条件\n",
    "   args:\n",
    "       X - 训练数据集\n",
    "       y - 目标标签值\n",
    "       w - 权重系数\n",
    "       step_init - 步长初始值\n",
    "       step_max - 步长最大值\n",
    "   return:\n",
    "       步长\n",
    "   \"\"\"\n",
    "    step_i = step_init\n",
    "    step_low = step_init\n",
    "    step_high = step_max\n",
    "    i = 1\n",
    "    d = dcost(X, y, w)\n",
    "    H = ddcost(X, y, w)\n",
    "    p = direction(d, H)\n",
    "    while (True):\n",
    "       # 不满足充分下降条件或者后面的代价函数值大于前一个代价函数值\n",
    "        if (not sufficientDecrease(X, y, w, step_i) or (cost(X, y, w + step_i * p) >= cost(X, y, w + step_low * p) and i > 1)):\n",
    "           # 将当前步长作为搜索的右边界\n",
    "           # return search(X, y, w, step_prev, step_i)\n",
    "            step_high = step_i\n",
    "        else:\n",
    "           # 满足充分下降条件并且满足曲率条件，即已经满足了 Wolfe 条件\n",
    "            if (curvature(X, y, w, step_i)):\n",
    "               # 直接返回当前步长\n",
    "                return step_i\n",
    "            step_low = step_i\n",
    "       # 选择下一个步长\n",
    "        step_i = select(step_low, step_high)\n",
    "        i = i + 1\n",
    "\n",
    "def logisticRegressionNewton(X, y, max_iter=1000, tol=1e-4, step_init=0, step_max=10):\n",
    "   \"\"\"\n",
    "   对数几率回归，使用牛顿法（newton's method）\n",
    "   args:\n",
    "       X - 训练数据集\n",
    "       y - 目标标签值\n",
    "       max_iter - 最大迭代次数\n",
    "       tol - 变化量容忍值\n",
    "       step_init - 步长初始值\n",
    "       step_max - 步长最大值\n",
    "   return:\n",
    "       w - 权重系数\n",
    "   \"\"\"\n",
    "   # 初始化 w 为零向量\n",
    "    w = np.zeros(X.shape[1])\n",
    "   # 开始迭代\n",
    "    for it in range(max_iter):\n",
    "       # 计算梯度\n",
    "        d = dcost(X, y, w)\n",
    "       # 计算黑塞矩阵\n",
    "        H = ddcost(X, y, w)\n",
    "       # 当梯度足够小时，结束迭代\n",
    "        if np.linalg.norm(d) <= tol:\n",
    "            break\n",
    "       # 使用线搜索计算步长 \n",
    "        step = lineSearch(X, y, w, step_init, step_max)\n",
    "       # 更新权重系数 w\n",
    "        w = w + step * direction(d, H)\n",
    "    return w\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a3b1feb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 假设数据\n",
    "X = np.array([[1], [2], [3], [4]])\n",
    "y = np.array([1, 1, 0, 0])\n",
    "logisticRegressionNewton(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a3fb742",
   "metadata": {},
   "source": [
    "__sklearn库__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b561a416",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-21.49011767]] [54.05874331]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# 初始化对数几率回归器，无正则化\n",
    "reg = LogisticRegression(penalty=\"none\")\n",
    "# 拟合线性模型\n",
    "reg.fit(X, y)\n",
    "# 权重系数\n",
    "w = reg.coef_\n",
    "# 截距\n",
    "b = reg.intercept_\n",
    "print(w,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "87a29b52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-2.24628215]] [5.38562353]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# 初始化对数几率回归器，L1正则化，使用坐标下降法\n",
    "reg = LogisticRegression(penalty=\"l1\", C=10, solver=\"liblinear\")\n",
    "# 拟合线性模型\n",
    "reg.fit(X, y)\n",
    "# 权重系数\n",
    "w = reg.coef_\n",
    "# 截距\n",
    "b = reg.intercept_\n",
    "print(w,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "325848fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-2.65138968]] [6.62851102]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# 初始化对数几率回归器，L2正则化\n",
    "reg = LogisticRegression(penalty=\"l2\", C=10)\n",
    "# 拟合线性模型\n",
    "reg.fit(X, y)\n",
    "# 权重系数\n",
    "w = reg.coef_\n",
    "# 截距\n",
    "b = reg.intercept_\n",
    "print(w,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "9d7a83e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.53395542]] [3.55844168]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# 初始化对数几率回归器，弹性网络正则化\n",
    "reg = LogisticRegression(penalty=\"elasticnet\", C=10, l1_ratio=0.5, solver=\"saga\")\n",
    "# 拟合线性模型\n",
    "reg.fit(X, y)\n",
    "# 权重系数\n",
    "w = reg.coef_\n",
    "# 截距\n",
    "b = reg.intercept_\n",
    "print(w,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e0762d74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.53395542]] [-39.61194806   5.82956778  33.78238028]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "X = np.array([[1], [2], [3], [4]])\n",
    "y = np.array([3, 1, 0, 0])\n",
    "# 初始化多分类对数几率回归器，无正则化\n",
    "reg = LogisticRegression(penalty=\"none\", multi_class=\"multinomial\")\n",
    "# 拟合线性模型\n",
    "reg.fit(X, y)\n",
    "# 权重系数\n",
    "W = reg.coef_\n",
    "# 截距\n",
    "b = reg.intercept_\n",
    "print(w,b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9899ebf2",
   "metadata": {},
   "source": [
    "## 决策树回归\n",
    "适用于大规模数据集，鲁棒性高"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b7a07d70",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2354b2b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 假设数据\n",
    "X = np.array([[1], [2], [3], [4]])\n",
    "y = np.array([2.5, 3.6, 3.4, 4.2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4f2ed668",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "模型深度： 3\n"
     ]
    }
   ],
   "source": [
    "# 初始化模型\n",
    "model = DecisionTreeRegressor()\n",
    "\n",
    "# 训练模型\n",
    "model.fit(X, y)\n",
    "\n",
    "# 输出结果\n",
    "print(\"模型深度：\", model.get_depth())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d224f41",
   "metadata": {},
   "source": [
    "## 回归问题"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55619154",
   "metadata": {},
   "source": [
    "__数据质量__  \n",
    "噪声数据：数据清洗，如中位数、平均数或高级算法进行填充。  \n",
    "缺失数据：使用插值方法或基于模型的预测来填充缺失值。\n",
    "\n",
    "__特征选择__  \n",
    "维度灾难：使用降维技术如 PCA 或特征选择算法。  \n",
    "共线性：使用正则化方法或手动剔除相关特征。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
